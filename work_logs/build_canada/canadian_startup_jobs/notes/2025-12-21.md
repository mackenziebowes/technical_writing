# Note Logs For The Day

## Planning

First thing this morning, after a restless weekend grappling with the tension of quite literally needing other humans to find me economically valuable and, after much research, realizing I have essentially zero control over how the neurons in the mind of "everyone else" (a set, not a person) actually fire, I started with a planning document.

The idea here is simple - I have a plan for the day, and I need to spend some time organizing my information and tools before I dive in so I can consistently work towards the end state - the job I'm trying to do being done.

Today's job is creating a `jobs` agent system for the Canadian Startup Jobs project.

This is a tricky system - jobs are unstructured data, and humans want to be able to search, sort, and filter that unstructured data. Primarily, all the LLM needs to do is take in unstructured data plus a schema and spit out structured date.

**Graphic**
(Unstructured Data + Schema) => Human-centered Job Listing
**/Graphic**

Due to the nature involved in parsing lists (here, "Job Boards" like a company's `/careers` page) and SQL, this actually has to happen in a couple steps.

**Graphic**
1. (JobBoard) => 2. (Create Job `jobs`) => 2.n (Unstructured Job HTML/md Payload) => 2.n.1 (Central Job Object Extracted), 2.n.2 (Job Tags Extracted) => 2.n.1.1 (Central Job Object Stored in DB), 2.n.2.1 (Job Tags Applied to 2.n.1.1)
**/Graphic**

We already have some of the logic about creating Job `jobs` from the earlier work in the `apps/cron` folder, my job is to handle the 2.n + tree - actually making the jobs searchable, sortable, and filterable.

The plan, then, is to do exactly that - handle everything beyond 2.n.

At this point, I had these planned assets:
```md
## Missing Assets

- **Job Board Reading Agent** - We need an agent that reads the career pages / job boards and uses the logic from the sibling cron package to add discovered pages to a queue.
- **Job Post Reading Agent** - We need an agent that reads each individual job post and returns data about it according to some prompt goal.
- **Job Post Storing System** - We need some system that preserves the details discovered by the reading Job Post Reading Agent
- **Job Post Tagging Agent** - We need an agent that reads the created job post, reads stored tags from the tag system, creates new tags if needed, and applies tags to jobs.

**Info:** We are splitting the Reading Agent and the Tagging Agent to limit tool-choice, a common failure vector for agentic systems.

- **Job Board Null Handling** - We need the Job Board Reading Agent to be able to return `null` jobs IFF there are no current careers
- **Org Tag Forwarding** - We need a way to forward the tags about `organizations` to the Job Post Tagging Agent as some data will be contextual. For example, if a job says "hybrid in our office" and doesn't list where the office is, we can use the `organizations` Province tag to resolve the `job` Province tag.
```

## First Pivot - Agent Clean Up

We originally had (agents colocated with their handler functions)[https://github.com/mackenziebowes/CanadianStartupJobs/blob/main/apps/server/src/lib/ai/functions/autoTagOrganization.ts]

This made my main tasks a bit difficult - I had to exctract the logic for various agentic systems to be applicable to the idea of Jobs, and these earlier scripts were highly interdependent and visually noisy. Some logic (Tagging agent results) could be duplicated, others (listTags) could not.

### Platonic Agents

This is a bad idea. Making a completely generic agent for a system with 3 fundamental data types is **massive over engineering** at the outset - REJECTED. It can remain as a refactoring goal for after the app actually works, not v1 stuff.

### Siloed Agents

Much better. We have 3 essential data types: Sources, Organizations, and Jobs. Each data type has it's own contextual needs (acceptable schemas, related tags), so each data type can get it's own agent (6 in total). That's manageable to write, and then we can identify repeated patterns and places where we may want hooks and extract that logic as needed.

created `./apps/server/src/lib/ai/agents/orgTaggingAgent.ts`.
extracted logic from `./apps/server/src/lib/ai/functions/autoTagOrganization.ts` to new file.

## Second Pivot - Tool Clean Up

Organizations and Jobs make use of different tags - makes no sense to apply a tag like `Job Type`, an enum for (Full Time | Part Time | Contract | Intern) to an organization.

### Overloading Tools

One solution here is to keep the idea of (listing and creating tags)[https://github.com/mackenziebowes/CanadianStartupJobs/blob/main/apps/server/src/lib/ai/tools/db.ts] as tools, but add a feature where the tool is different based on some key applied to a "constructing function" (I have no idea what the technical term is for this) like so:

#### What we had
```ts
const tagNameSchema = z
  .enum(["Team Size", "Raising Stage", "Province", "Industry"])
  .describe("The readable name of a tag field");

const listTagSchema = z.object({
  tagName: tagNameSchema,
  skip: z.number().describe("Pagination Offset"),
  take: z.number().describe("List Size"),
});
type ListTag = z.infer<typeof listTagSchema>;

export const listTags = tool({
  description: "List existing tags in a tag field.",
  inputSchema: listTagSchema,
  execute: async ({ tagName, skip, take }: ListTag) => {
    logGeneric("List Tags", { tagName, skip, take });
    switch (tagName) {
      case "Team Size":
        return JSON.stringify(await teamSizeCRUD.read(skip, take));
      case "Raising Stage":
        return JSON.stringify(await raisingStageCRUD.read(skip, take));
      case "Province":
        return JSON.stringify(await provincesCRUD.read(skip, take));
      case "Industry":
        return JSON.stringify(await industriesCRUD.read(skip, take));
    }
  },
});
```
#### Proposed Overload
```ts
const orgTagNameSchema = z
  .enum(["Team Size", "Raising Stage", "Province", "Industry"])
  .describe("The readable name of a tag field");

const listOrgTagSchema = z.object({
  tagName: orgTagNameSchema,
  skip: z.number().describe("Pagination Offset"),
  take: z.number().describe("List Size"),
});
type ListOrgTag = z.infer<typeof listOrgTagSchema>;

const jobTagNameSchema = z
  .enum(["Province", "Industry", "Experience Level", "Employment Type", "Role"])
  .describe("The readable name of a tag field");

const listJobTagSchema = z.object({
  tagName: jobTagNameSchema,
  skip: z.number().describe("Pagination Offset"),
  take: z.number().describe("List Size"),
});
type ListJobTag = z.infer<typeof listJobTagSchema>;

export const listTags = (type: "org" | "job") => {
  if (type == "org") {
    return tool({
      description: "List existing tags in a tag field.",
      inputSchema: listOrgTagSchema,
      execute: async ({ tagName, skip, take }: ListOrgTag) => {
        logGeneric("List Tags", { tagName, skip, take });
        switch (tagName) {
          case "Team Size":
            return JSON.stringify(await teamSizeCRUD.read(skip, take));
          case "Raising Stage":
            return JSON.stringify(await raisingStageCRUD.read(skip, take));
          case "Province":
            return JSON.stringify(await provincesCRUD.read(skip, take));
          case "Industry":
            return JSON.stringify(await industriesCRUD.read(skip, take));
        }
      },
    });
  }
  if (type == "job") {
    /* You get the picture */
  }
};
```

This is stupid as fuck basically. Massively increases visual noise, makes it way hard to find where edits actually need to go, etc. This sucks. If someone did this in my team, I would be hopping mad.

This kind of thing makes sense in the case of switch statements - if there were many Primary Objects with unique details, you would use a switch case to return the right tool using an enum to simplify writing when constructing the agent.

In that case, you still wouldn't do it quite like this - every tool would declare it's schemas, types, and handlers elsewhere as a unique tool and then return that subfunction in the larger switch case. This proposal is a mess, REJECTED.

### Better: Dot Syntax / Barrel Exporting

To give away the end of this system, here's what an improved agent declaration looks like:

```ts
import { Experimental_Agent as Agent, stepCountIs } from "ai";
import { google } from "@ai-sdk/google";
import { readPage, searchSite, orgTools } from "@/lib/ai/tools";
import { observePrepareSteps } from "../observability";

export const orgTaggingAgent = new Agent({
  model: google("gemini-2.5-pro"),
  tools: {
    readPage,
    searchSite,
    listTags: orgTools.tags.list,
    createTag: orgTools.tags.create,
    connectOrgToTag: orgTools.tags.connect
  },
  stopWhen: stepCountIs(10),
  prepareStep: observePrepareSteps("Organization Tagging")
});
```

These tools are complex - they have to have, as mentioned, unique schemas, types, and handlers already. We have to store that complexity somewhere, and manage the readability and discoverability of those functions as the product matures.

Barrel exports are perfect for that - it's not exactly DRY, these definitely aren't monoids or anything fancy, but right now we're trying to get to a v0 that can be placed in front of users to discover what needs changing.
We're also trying to discover how to harden agents and solve problems like reducing hallucinations and hardening data.

As we move into the future of this project, this kind of siloed system makes it relatively simple to track changes throughout the project - if we need to add a tag to organizations, easy, we just update the enums in the `tools/db/organizations` file. If we need to add a tag to jobs, same story.

*Importantly*, this brings us back on track faster - the fact that the ergonomics for adding tags to jobs is identical allows us to focus on satisfying the Job Tag Schema *alone* without pixel-hunting where a new schema would go and spellchecking names like listJobsTagsSchema that are hostile to humans to begin with. It's just `listTagsSchema` everywhere - one verb, one noun, one modifier - as simple as it gets.

I know it doesn't *look* simple to a non-engineer, but that's always my top priority - K.I.S.S. so I don't hurt myself later on.

## Dot Syntax Retrospective

I got the basic dot-syntax idea down for job tagging, based on the patterns I recently used for adding the organization tagging.

In this process, there were a few other sub pivots that were needed:

### Seeding

In this repository, in packages/db/src/scripts, there is a `seed.ts` which is for adding data that doesn't change much during runtime and would be required for normal functioning of various systems.

When I started this morning, only `provinces` were in that file - Readable Names and Province Codes.

We added `jobTypes` and `experienceLevels`.

Job Types are meant to be filters for... the job type. "Kind" of employment.
This list is open for expansion, but not by the AI - the idea of `jobType` is likely to be a bit hostile to an LLM due to the semantic overload of the idea of "types" - I expect an LLM to confuse between employment type, industry, and role frequently.
Arguably, philosophically, working any given role makes a unique "type" or "kind" of job. You can be working a "hospitality" type job, or a "marketing" type job.

This filter is for filtering on, essentially, commitment levels from the job seeker, so we added an exclusive list:

```ts
const JOB_TYPES = [
  { name: "Full Time" },
  { name: "Part Time" },
  { name: "Internship" },
  { name: "Contract" }
];
```

This may need updates later - whatever - the point is to hand the LLM examples of what the heck a `jobType` actually is, and to hand the users tooling to filter the available jobs by their desired level of commitment.

Experience Levels are similarly likely to be problematic for LLMs - we would prefer that the LLM is translating Job Requirements and Job Duties into an Experience Level based on, yknow, the latent space inside the LLM rather than simply duplicate out the level that the job lists.

However... there's some category issues in this project, **over all**, in that it's merely a **Federal Job Board**.
This isn't a federal **Tech** job board, a federal **Law** job board, a federal **Design** job board, or any other kind of `role` type.

```ts
const EXPERIENCE_LEVELS = [
  { name: "Entry" },
  { name: "Mid" },
  { name: "Senior" },
  { name: "Staff" },
  { name: "Principal" },
  // ^ Tech, Design, general HCI jobs
  { name: "Director" },
  { name: "Executive" },
  { name: "C-Suite" },
  // ^ Generic Corporate Jobs
  { name: "Associate" },
  // ^ This one is a bit awkward - applies to corporate and many more
  { name: "Junior Partner" },
  { name: "Partner" },
  { name: "Lead" },
  // ^ Legal Jobs
];
```

## Job Creation

At first, I'm a bit lost on approaching this feature.

LLM systems, in general, tend to compress each given topic area of their output in order to hit a relatively standardized `tokens_per_round` metric. The more topics you request or encourage the LLM to address per turn, the shorter and less detailed each given topic is in the overall message.

So, I'm worrying about having the LLM phone it in when creating a job in a single shot - I'm wondering if we'll get better data if we separate out some topics that need more detail, or benefit from increased attention, that we can organize into some kind of tool-call data harness.

The hypothetical structure flows something like this:

**Graphic**
(call: writeDescription(`Long String Description`)) => (`Long String Description` temporarily stored in memory) => (call: saveJob()) => (`Long String Description` pulled from memory and passed to DB automatically if set, error save call if not set)
**/Graphic**

Let's look at the autogenerated schema to see if there's even things we need to have detail:

```md
## jobs (`jobs`)

| Column | Type | Required | Default |
| :--- | :--- | :--- | :--- |
| `id` | `number` | Yes | - |
| `title` | `string` | Yes | - |
| `city` | `string` | Yes | - |
| `province` | `string` | Yes | - |
| `remote_ok` | `boolean` | Yes | - |
| `salary_min` | `number` | No | - |
| `salary_max` | `number` | No | - |
| `description` | `string` | Yes | - |
| `company` | `string` | Yes | - |
| `job_board_url` | `string` | No | - |
| `posting_url` | `string` | No | - |
| `is_at_a_startup` | `boolean` | No | - |
| `last_scraped_markdown` | `string` | No | - |
| `created_at` | `date` | Yes | `[object Object]` |
| `updated_at` | `date` | Yes | `[object Object]` |
```

Right, yes, this data type has some problems.

### Salary
I, perhaps foolishly, made salary_min and salary_max optional. The thinking here was to store progress as a human user worked through a form - there's a key UX principle of not requiring humans to repeat themselves throughout a form, and this rough structure enables us to hand a user who works for a specific company the ability to `add job` and have that new job be prefilled with data about their company that we're guaranteed to have - city, province, company name.

This way, step 1 just has to have 3 unique data entries: Title, Remote Tolerance (yes/no), and description. Step 2 is setting the salary for the position. If the job creator never chooses to list a salary, like some absolute weirdos that exist in this country enjoy doing, then they are able to exercise their weirdo right to do that without any further judgement from me. Trying not to be a black pot here.

Anyways - *certainly* it doesn't look like there are any requirements that need to be explicitly long or detailed, but we may encounter the previous issue around salary min and max being optional meaning the agent doesn't have to fill them in.

### Synthetic Schemas?

We may make a synthetic schema here - giving the LLM the option to explicitly mark a `salaryHidden` boolean that removes `salary_min` and `salary_max` from the required list, and throw on something like a `saveJob` toolcall if all three values are unset.

This has other benefits, too - the ideas about `company`, `province`, `posting_url`, `job_board_url`, and `is_at_a_startup` are opportunities for us to apply that UX principle when considering the LLM as a User of our data entry system - we don't need it to repeat itself!

Let's try it out!
