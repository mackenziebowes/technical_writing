# Roadmap: December 17, 2025

(This report was AI Generated from a coding agent)

**Goal:** End-to-end data flow: `Source` → `Portfolio` → `Organization` → `Job`.
**Constraint:** "Cache and track" at every step (hashing content to detect changes).

## Phase 1: Organization Engine ("The Heavy Lift")

### 1. Job Board Cache
*   **Action:** Implement `createNewJobBoardCache` in `apps/server/src/lib/ai/functions/createNewJobBoardCache.ts`.
*   **Logic:** Fetch URL → Hash Content → Insert to `jobBoardCaches`.
*   **Ref:** Similar to `createNewPortfolioCache.ts`.

### 2. Organization Parsing & Tagging
*   **Action:** Implement `createNewOrganizationFromMarkdown`.
*   **Complex Requirement:** Zod schemas must allow AI to select *extant* tags (Industries, Provinces, etc.) from the DB.
*   **Helpers Needed:** Retrieve current tags from DB to populate Zod enums/arrays dynamically for the prompt.

### 3. Connection
*   **Action:** Connect `Organization` ↔ `JobBoardCache`.
*   **Table:** `orgsJobBoardCaches` (pivot).

## Phase 2: Job Engine ("The Adaptation")

### 1. Job Cache
*   **Action:** Implement `createNewJobCache`.
*   **Target Schema:** `jobCaches` (in `jobs/index.ts`).
*   **Logic:** Fetch specific job post → Hash → Store.

### 2. Job Parsing & Tagging
*   **Action:** Implement `createNewJobFromMarkdown`.
*   **Requirement:** Tagging support (Roles, Job Types, Experience Levels).

### 3. Connection
*   **Action:** Link `Job` ↔ `JobCache`.
*   **Action:** Link `Organization` ↔ `Job` (via `orgsJobs` pivot?).

## Definition of Done (Step 1)
1.  Can create a new `Organization`.
2.  Can create a new `JobBoardCache` (hashed content).
3.  Can connect them via pivot table.
4.  **AI Parsing includes Tagging:** The creation process correctly identifies and links existing tags (Industry, Province, Stage, Size) based on the input markdown.

## Context Notes
*   **DB:** `organizations`, `jobBoardCaches`, `orgs*` pivots.
*   **Existing Helpers:** `createNewPortfolioCache.ts` is the template.
*   **Validation:** Use `zod` schemas that mirror the DB structure but enforced for AI generation.
*   **Optimization Idea (Post-MVP):** The current flow asks the LLM to output the URL it found. We should automate this by passing the URL context directly to the saver function to save tokens and improve reliability.
*   **Missing System:** AI Monitoring Harness. We need to track token usage, costs, and model performance. Add to post-MVP list.
*   **Missing System:** Logging Harness. Abstract `console.log` into a service that can route logs to KV/External Storage in production (via `LOG_STORAGE_URL`) or Console in dev. Needs standardized metadata (timestamp, severity).
